Overview:

a. 1 large Icarus MAAS node, with Bionic deployed via the MAAS web UI.
b. 6 KVM guests created with virt-install on this machine:
	- 1 for the MAAS server
	- 1 for the Juju controller
	- 4 for the cloud nodes
c. Icarus node, beyond hosting KVM guests, will act as:
	- Juju client
	- OpenStack client
d. 2 libvirt networks will be used:
	- 'external' for the external side of the MAAS server (DHCP enabled)
	- 'internal' for the internal side of the MAAS server (DHCP disabled)


SSH to Icarus node with agent forwarding enabled
(this can help with connectivity as uvtool can auto-install the agent's keys on its created instances) 

$ ssh -A <icarus-ip-address>


cd
sudo apt install -y uvtool virtinst
sudo uvt-simplestreams-libvirt sync release=focal arch=amd64 (this only affects the Ubuntu release of the MAAS server)
sudo snap install juju --classic
sudo snap install charm --classic
sudo snap install openstackclients --classic
charm pull openstack-base

Log out and back in again. Ensure the 'default' libvirt network exists:
$ virsh net-list --all


--------------------------------------
OPTIONAL: USE ZFS POOLS

sudo apt install -y zfsutils-linux libvirt-daemon-driver-storage-zfs
sudo systemctl restart libvirtd

lsblk
echo 1 | sudo tee /sys/block/bcache0/bcache/detach
echo 1 | sudo tee /sys/block/bcache0/bcache/stop
echo 1 | sudo tee /sys/block/bcache1/bcache/detach
echo 1 | sudo tee /sys/block/bcache1/bcache/stop
echo 1 | sudo tee /sys/block/bcache2/bcache/detach
echo 1 | sudo tee /sys/block/bcache2/bcache/stop
sudo wipefs -a /dev/sdb
sudo wipefs -a /dev/sdc
sudo wipefs -a /dev/sdd

sudo zpool create kvm-zfs sdb sdc sdd

sudo zfs create kvm-zfs/images
virsh pool-define-as --name images --source-name kvm-zfs/images --type zfs

virsh pool-start images
virsh pool-autostart images

sudo chmod a+x /dev/zvol/kvm-zfs/images (fails)

zfs list -t all
NAME             USED  AVAIL     REFER  MOUNTPOINT
kvm-zfs          160K  2.63T       24K  /kvm-zfs
kvm-zfs/images    24K  2.63T       24K  /kvm-zfs/images

virsh pool-list
 Name      State    Autostart
-------------------------------
 libvirt   active   yes
 uvtool    active   yes
--------------------------------------

(check above for uvtool)

Create the libvirt networks:

$ cat net-external.xml 
<network>
  <name>external</name>
  <uuid>790274ec-2590-4854-b432-ea7d22deb667</uuid>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:c9:86:40'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254'/>
      <host mac='52:54:00:01:01:01' ip='192.168.122.2' />
    </dhcp>
  </ip>
</network>
--------------------


Consider using subnet 10.5.0.0/16 as this is what is used in my ServerStack
private subnet (easy to manage VIPs in bundles; they can be identical).

$ cat net-internal.xml 
<network>
  <name>internal</name>
  <uuid>790274ec-2590-4854-b432-ea7d22deb668</uuid>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr1' stp='on' delay='0'/>
  <mac address='52:54:00:c9:86:41'/>
  <ip address='10.0.0.1' netmask='255.255.255.0'>
  </ip>
</network>
--------------------


$ cat create-networks.sh
#!/bin/sh -e

virsh net-destroy default
virsh net-undefine default

virsh net-define net-external.xml
virsh net-start external
virsh net-autostart external

virsh net-define net-internal.xml
virsh net-start internal
virsh net-autostart internal
--------------------


cd install-maas
./create-networks.sh


$ cat template-maas.xml 
<domain type='kvm'>
  <os>
    <type>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <devices>
    <interface type='network'>
      <source network='external'/>
      <model type='virtio'/>
      <mac address='52:54:00:01:01:01'/>
    </interface>
    <interface type='network'>
      <source network='internal'/>
      <model type='virtio'/>
      <mac address='52:54:00:01:01:02'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/3'/>
      <target port='0'/>
    </serial>
    <graphics type='vnc' autoport='yes' listen='127.0.0.1'>
      <listen type='address' address='127.0.0.1'/>
    </graphics>
    <video/>
  </devices>
</domain>
--------------------



Launch a test instance to discover what names the two network interfaces (created via template-maas.xml) will have:

uvt-kvm create --template ./template-maas.xml test release=focal
uvt-kvm ssh test ip a
uvt-kvm destroy test



The cloud-init file for the MAAS server:

cat user-data-test.yaml

--------------------
#cloud-config
ssh_pwauth: yes
chpasswd:
  list: |
    ubuntu:ubuntu
  expire: False

write_files:
  - path: /etc/netplan/50-cloud-init.yaml
    owner: root:root
    permissions: '0644'
    content: |
      network:
        version: 2
        ethernets:
          enp1s0:
              dhcp4: true
              match:
                  macaddress: '52:54:00:01:01:01'
              set-name: enp1s0
          enp2s0:
              dhcp4: false
              match:
                  macaddress: '52:54:00:01:01:02'
              set-name: enp2s0
              addresses:
                - 10.0.0.2/24

runcmd:
  - snap install maas-test-db --channel=2.8/candidate
  - snap install maas --channel=2.8/candidate
  - maas init region+rack --maas-url http://10.0.0.2:5240/MAAS --database-uri maas-test-db:///
  - maas createadmin --username admin --password ubuntu --email admin@example.com --ssh-import lp:petermatulis
  - maas apikey --username admin > ~ubuntu/admin-api-key
  - mkdir -p /var/snap/maas/current/root/.ssh
  - ssh-keygen -q -N '' -f /var/snap/maas/current/root/.ssh/id_rsa
  - ssh-import-id lp:petermatulis
  - su - ubuntu -c "ssh-import-id lp:petermatulis"

power_state:
  mode: reboot
--------------------


Create the MAAS server:

uvt-kvm create \
   --template ./template-maas.xml \
   --user-data ./user-data-maas.yaml \
   --cpu 4 --memory 4096 --disk 30 maas \
   release=focal


Wait about 4 minutes...
ssh ubuntu@10.0.0.2 tail -f /var/log/cloud-init-output.log


Get the MAAS 'admin' user's API key:

scp ubuntu@10.0.0.2:admin-api-key ~


Install the 'root' user public key in the 'ubuntu' user account on the Icarus node
and confirm that the 'root' user can query the KVM host:

ssh root@10.0.0.2 cat /var/snap/maas/current/root/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

--------
Test MAAS:KVM connectivity:

ssh ubuntu@10.0.0.2
sudo snap run --shell maas
virsh -c qemu+ssh://ubuntu@10.0.0.1/system list --all
exit
exit
--------

Access the web UI for the first time:

ssh -N -L 8002:10.0.0.2:5240 ubuntu@<icarus-ip-address> (from desktop)

http://localhost:8002/MAAS

credentials: admin/ubuntu

Configure for images (e.g. amd64 for Bionic and Focal)

1. DNS for subnet: 10.0.0.1 (not forwarder)
2. Gateway for subnet: 10.0.0.1
3. Enable DHCP for the 'untagged' VLAN
4. Set up Reserved IP ranges:

   10.0.0.1   - 10.0.0.9     Infra
   10.0.0.10  - 10.0.0.99    VIP	<-- HA workloads
   10.0.0.100 - 10.0.0.119   Dynamic   	<-- DHCP (enlistment, commissioning)

   So Deployed nodes will use:
   
   10.0.0.120 - 10.0.0.254

4. Verify controller status ('regiond' to 'dhcpd' should be green)

To make stuff green:

$ ssh ubuntu@10.0.0.2 sudo systemctl restart maas-rackd.service
$ ssh ubuntu@10.0.0.2 sudo systemctl restart maas-regiond.service



$ cat create-controller.sh 
#!/bin/sh -e

VARIANT=ubuntu18.04
VCPUS=2
RAM_SIZE_MB=4000
DISK_SIZE_GB_1=30
NAME=controller
MAC1="52:54:00:02:01:01"

virt-install \
	--os-variant $VARIANT \
	--graphics vnc \
	--noautoconsole \
	--network network=internal,mac=$MAC1 \
	--name $NAME \
	--vcpus $VCPUS \
	--cpu host \
	--memory $RAM_SIZE_MB \
	--disk "$NAME"_1.img,size=$DISK_SIZE_GB_1 \
	--boot network



$ cat create-nodes.sh 
#!/bin/sh -e

VARIANT=ubuntu18.04
VCPUS=9
RAM_SIZE_MB=62000
DISK_SIZE_GB_1=30
DISK_SIZE_GB_2=30
DISK_SIZE_GB_3=30

for NAME in node1 node2 node3 node4; do

        case $NAME in
        node1)
          MAC1="52:54:00:03:01:01"
          MAC2="52:54:00:03:01:02"
          ;;
        node2)
          MAC1="52:54:00:03:02:01"
          MAC2="52:54:00:03:02:02"
          ;;
        node3)
          MAC1="52:54:00:03:03:01"
          MAC2="52:54:00:03:03:02"
          ;;
        node4)
          MAC1="52:54:00:03:04:01"
          MAC2="52:54:00:03:04:02"
          ;;
        esac

        virt-install \
                --os-variant $VARIANT \
                --graphics vnc \
                --noautoconsole \
                --network network=internal,mac=$MAC1 \
                --network network=internal,mac=$MAC2 \
                --name $NAME \
                --vcpus $VCPUS \
                --cpu host \
                --memory $RAM_SIZE_MB \
                --disk "$NAME"_1.img,size=$DISK_SIZE_GB_1 \
                --disk "$NAME"_2.img,size=$DISK_SIZE_GB_2 \
                --disk "$NAME"_3.img,size=$DISK_SIZE_GB_3 \
                --boot network

done
------------------------



Create the five nodes:

./create-controller.sh
./create-nodes.sh


zfs list -t all
NAME                         USED  AVAIL     REFER  MOUNTPOINT
kvm-zfs                      420K  2.63T       25K  /kvm-zfs
kvm-zfs/libvirt              180K  2.63T       24K  /kvm-zfs/libvirt
kvm-zfs/libvirt/controller    12K  2.63T       12K  -
kvm-zfs/libvirt/node1         12K  2.63T       12K  -
kvm-zfs/libvirt/node1-1       12K  2.63T       12K  -
kvm-zfs/libvirt/node1-2       12K  2.63T       12K  -
kvm-zfs/libvirt/node2         12K  2.63T       12K  -
kvm-zfs/libvirt/node2-1       12K  2.63T       12K  -
kvm-zfs/libvirt/node2-2       12K  2.63T       12K  -
kvm-zfs/libvirt/node3         12K  2.63T       12K  -
kvm-zfs/libvirt/node3-1       12K  2.63T       12K  -
kvm-zfs/libvirt/node3-2       12K  2.63T       12K  -
kvm-zfs/libvirt/node4         12K  2.63T       12K  -
kvm-zfs/libvirt/node4-1       12K  2.63T       12K  -
kvm-zfs/libvirt/node4-2       12K  2.63T       12K  -


In the MAAS web UI:

SEE config-nodes.sh FOR STEPS #1 and #2

1. Rename the 5 nodes to match their libvirt domain names
Use the MAC1 addresses as a mapping:

(01:01:01 maas)
02:01:01 controller
03:01:01 node1
03:02:01 node2
03:03:01 node3
03:04:01 node4

2. Configure the power type for the 5 nodes:
type: Virsh
Virsh address: qemu+ssh://ubuntu@10.0.0.1/system
Virsh VM ID: <libvirt-domain/node-name> (e.g. node1)

3. Give each MAC1 interface an IP mode of 'Auto assign' (10.0.0.0/24 subnet)
4. Give each MAC2 interface (for nodeX) a state of 'Unconfigured' (on same
   fabric as in #3?)
5. Give a tag of 'juju' to the controller node
6. Commission all 5 nodes


Define a MAAS cloud, add it to Juju, and add a credential to the cloud:

$ cat add-cloud-and-creds.sh
#!/bin/sh -e

MAAS_INTERNAL=10.0.0.2
CLOUD_YAML=~/cloud.yaml
CREDS_YAML=~/credentials.yaml
CLOUD_NAME=mymaas
CREDS_NAME=anyuser
API_KEY=$(cat ~/admin-api-key)

cat > $CLOUD_YAML << HERE
clouds:
  $CLOUD_NAME:
    type: maas
    auth-types: [oauth1]
    endpoint: http://$MAAS_INTERNAL:5240/MAAS
HERE

cat > $CREDS_YAML << HERE
credentials:
  $CLOUD_NAME:
    $CREDS_NAME:
      auth-type: oauth1
      maas-oauth: $API_KEY
HERE

juju add-cloud --client $CLOUD_NAME $CLOUD_YAML
juju add-credential --client -f $CREDS_YAML $CLOUD_NAME

echo
echo "To create the Juju controller for cloud $CLOUD_NAME:"
echo
echo "juju bootstrap --bootstrap-constraints tags=juju mymaas maas-controller"
-------------------------



$ ./add-cloud-and-creds.sh



Create the Juju controller:

$ juju bootstrap --bootstrap-constraints tags=juju mymaas maas-controller



Install and configure OpenStack:

https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/install-openstack.html

In particular, confirm the contents of these application configuration files:

- ceph-osd.yaml
- neutron.yaml
- swift-storage.yaml

Notes:

$ watch -n 5 -c juju status --color
$ juju debug-log -m <model-name>
$ source ~/openstack-base/openrc


config-nodes.sh
+++++++++++++++++++++++++++++++++++++
#!/bin/bash                                                                                                                             
                                                                                                                                        
#set -e                                                                                                                                 
                                                                                                                                        
PROFILE=admin
API_KEY_FILE=/home/ubuntu/admin-api-key
KVM_EXTERNAL_IP=10.0.0.1
                                                                                                                                                                                                                          
declare -A nodeNamesMACs=( \                                                                                                            
        [node1]=52:54:00:03:01:01 \                                                                                                     
        [node2]=52:54:00:03:02:01 \                                                                                                     
        [node3]=52:54:00:03:03:01 \                                                                                                     
        [node4]=52:54:00:03:04:01 \
        [controller]=52:54:00:02:01:01 \                                                                                                     
        )                                                                                                                               
                                                                                                                                        
maas login $PROFILE http://localhost:5240/MAAS - < $API_KEY_FILE >/dev/null                                                          
                                                                                                                                        
maas $PROFILE tags create name=juju comment='Juju controller' >/dev/null && echo -ne "\nMAAS tag 'controller' created"            
                                                                                                                                        
# For each KVM guest node:                                                                                                              
for i in "${!nodeNamesMACs[@]}"; do                                                                                                    
        echo -e "\nConfiguring node $i"                                                                                                
        MAC1=${nodeNamesMACs[$i]}                                                                                                     
        SYSTEM_ID=$(maas $PROFILE machines read mac_address=$MAC1 | grep -i system_id -m 1 | cut -d '"' -f 4)                         
        maas $PROFILE machine update $SYSTEM_ID \                                                                                      
                hostname=$i \                                                                                                          
                power_type=virsh \                                                                                                      
                power_parameters_power_address=qemu+ssh://ubuntu@"$KVM_EXTERNAL_IP"/system \                                            
                power_parameters_power_id=$i >/dev/null && echo "- Node name changed and power type configured"                        
        maas $PROFILE machine commission $SYSTEM_ID testing_scripts=none >/dev/null && echo "- Node commissioning (hardware tests skipped)"
                                                                                                                                        
        # Node 'controller' is the Juju controller, apply tag 'juju'
        if [ $i = "controller" ]; then                                                                                                      
                 maas $PROFILE tag update-nodes controller add=$SYSTEM_ID >/dev/null && echo "- Tag 'juju' assigned to node $i"       
        fi                                                                                                                              
                                                                                                                                        
done
+++++++++++++++++++++++++++++++++++++

maas $PROFILE ipranges create type=reserved start_ip=$MAAS_IP_RESERVED_RANGE_LOW end_ip=$MAAS_IP_RESERVED_RANGE_HIGH >/dev/null && echo "Reserved IP range set"
maas $PROFILE ipranges create type=dynamic start_ip=$MAAS_IP_DYNAMIC_RANGE_LOW end_ip=$MAAS_IP_DYNAMIC_RANGE_HIGH >/dev/null && echo "Dynamic IP range set"
# Enable DHCP (can only be done after creating a dynamic range above)                                                                   
FABRIC_ID=\$(maas $PROFILE subnet read $INSTANCE_SUBNET_CIDR | grep fabric- -m 1 | cut -d ' ' -f 10 | cut -d '"' -f 2)                  
maas $PROFILE vlan update \$FABRIC_ID untagged dhcp_on=True primary_rack=$MAAS_INSTANCE >/dev/null && echo "DHCP enabled on \$FABRIC_ID"
maas $PROFILE subnet update $INSTANCE_SUBNET_CIDR gateway_ip=$MAAS_INTERNAL_IP >/dev/null && echo "Default gateway set"                 
# Set the DNS server for the internal network                                                                 
maas $PROFILE subnet update $INSTANCE_SUBNET_CIDR dns_servers=$MAAS_INTERNAL_IP >/dev/null && echo "DNS server set" 

FABRIC_ID=$(maas admin subnet read 10.0.0.0/24 | grep fabric- -m 1 | awk '{print $2}' | cut -d '"' -f 2)
maas admin vlan update fabric-1 untagged dhcp_on=True primary_rack=10.0.0.2 >/dev/null && echo "DHCP enabled on untagged VLAN on $FABRIC-ID"
maas admin ipranges create type=reserved start_ip=10.0.0.1 end_ip=10.0.0.9 comment="Infra" >/dev/null && echo "Reserved IP range set (Infra)"
maas admin ipranges create type=dynamic start_ip=10.0.0.10 end_ip=10.0.0.99 >/dev/null && echo "Dynamic IP range set (DHCP)"
maas admin ipranges create type=reserved start_ip=10.0.0.100 end_ip=10.0.0.119 comment="VIP" >/dev/null && echo "Reserved IP range set (VIP)"
maas admin subnet update 10.0.0.0/24 gateway_ip=10.0.0.1 >/dev/null && echo "Default gateway set for subnet 10.0.0.0/24"
maas admin subnet update 10.0.0.0/24 dns_servers=10.0.0.1 >/dev/null && echo "DNS server set for subnet 10.0.0.0/24"


